<!--- SPDX-License-Identifier: Apache-2.0 -->

# GRU

## Description

GRU, introduced by Cho et al. in 2014, is designed to solve the vanishing gradient problem faced by standard recurrent neural networks (RNN). Similarly to the LSTM unit, GRU uses a gating mechanism to control the memorization process.

## Model

|Model            |Download                        |
|-----------------|:-------------------------------|
| GRU(pytorch)    |[18MB](gru.pt)                  |
| GRU(onnx)       |[18MB](gru.onnx)                |

## Dataset

* [SST](https://nlp.stanford.edu/sentiment/index.html)

## References

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)
* [bamtercelboo/cnn-lstm-bilstm-deepcnn-clstm-in-pytorch](https://github.com/bamtercelboo/cnn-lstm-bilstm-deepcnn-clstm-in-pytorch)

## License

Apache 2.0
